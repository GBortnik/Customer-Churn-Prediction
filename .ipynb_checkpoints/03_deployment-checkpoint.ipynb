{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71a16a46-9b06-4715-99f2-61241d9727c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.46.1-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas<3,>=1.4.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from streamlit) (1.5.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from streamlit) (9.2.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from streamlit) (4.25.6)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from streamlit) (14.0.2)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from streamlit) (5.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from streamlit) (2.32.3)\n",
      "Collecting altair<6,>=4.0\n",
      "  Downloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.2/731.2 kB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: blinker<2,>=1.5.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: packaging<26,>=20 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from streamlit) (25.0)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from streamlit) (4.13.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from streamlit) (8.1.8)\n",
      "Collecting watchdog<7,>=2.1.5\n",
      "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<3,>=1.23 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from streamlit) (1.23.5)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from streamlit) (3.1.44)\n",
      "Collecting pydeck<1,>=0.8.0b4\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: jinja2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (1.35.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2022.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (1.26.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.0.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.19.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Installing collected packages: watchdog, pydeck, altair, streamlit\n",
      "Successfully installed altair-5.5.0 pydeck-0.9.1 streamlit-1.46.1 watchdog-6.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1562c12d-8a2e-4a71-b7b1-462272a9a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "131fc158-c490-4e5c-836e-787bd58512d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "streamlit: 1.46.1\n",
      "joblib: 1.2.0\n",
      "pandas: 1.5.3\n",
      "numpy: 1.23.5\n",
      "plotly: 6.1.1\n",
      "sklearn: 1.5.1\n"
     ]
    }
   ],
   "source": [
    "print(\"streamlit:\", st.__version__)\n",
    "print(\"joblib:\", joblib.__version__)\n",
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"plotly:\", plotly.__version__)\n",
    "print(\"sklearn:\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "818b06d3-5340-4778-801d-73a52b2e6502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requirements.txt created\n"
     ]
    }
   ],
   "source": [
    "# preparing file for streamlit environment\n",
    "requirements = '''streamlit==1.46.1\n",
    "joblib==1.2.0\n",
    "pandas==1.5.3\n",
    "numpy==1.23.5\n",
    "plotly==6.1.1\n",
    "scikit-learn==1.5.1\n",
    "'''\n",
    "# Save requirements.txt \n",
    "with open('requirements.txt', 'w') as f:\n",
    "    f.write(requirements)\n",
    "print(\"requirements.txt created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cf92b1b-9243-48e2-abb3-d24f4d77e85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app.py created\n"
     ]
    }
   ],
   "source": [
    "# Creating app.py file\n",
    "app = '''import streamlit as st\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "\n",
    "# Page config\n",
    "st.set_page_config(\n",
    "    page_title=\"Churn Prediction App\",\n",
    "    page_icon=\"📊\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "def preprocess_new_data(new_data, preprocessing_info):\n",
    "    \"\"\"\n",
    "    Preprocess new data the same way as train data\n",
    "    \"\"\"\n",
    "    df = new_data.copy()\n",
    "    \n",
    "    # Remove ID cols\n",
    "    df = df.drop(columns=[col for col in preprocessing_info['id_cols'] if col in df.columns], errors='ignore')\n",
    "    \n",
    "    # Label encoding binary columns\n",
    "    le = LabelEncoder()\n",
    "    for col in preprocessing_info['bin_cols']:\n",
    "        if col in df.columns and col != 'Churn':  # exclude target if present\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "    \n",
    "    # get_dummies for multi-values columns\n",
    "    df = pd.get_dummies(data=df, columns=preprocessing_info['multi_cols'])\n",
    "    \n",
    "    # Make sure we have all dummy columns (add missing ones with 0 values\n",
    "    for col in preprocessing_info['final_feature_names']:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "    \n",
    "    # Scaling numerical columns\n",
    "    if preprocessing_info['num_cols']:\n",
    "        scaled_nums = preprocessing_info['scaler'].transform(df[preprocessing_info['num_cols']])\n",
    "        scaled_df = pd.DataFrame(scaled_nums, columns=preprocessing_info['num_cols'], index=df.index)\n",
    "        \n",
    "        # Replace original numerical columns with scaled ones\n",
    "        df = df.drop(columns=preprocessing_info['num_cols'])\n",
    "        df = df.merge(scaled_df, left_index=True, right_index=True, how=\"left\")\n",
    "    \n",
    "    df = df[preprocessing_info['final_feature_names']]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def predict_churn(input_data, pipeline_path='churn_model_pipeline.joblib'):\n",
    "    \"\"\"\n",
    "    Complete predict function - from raw data to result\n",
    "    \"\"\"\n",
    "    # Load pipeline\n",
    "    pipeline = joblib.load(pipeline_path)\n",
    "    \n",
    "    # Preprocess data\n",
    "    processed_data = preprocess_new_data(input_data, pipeline['preprocessing_info'])\n",
    "    \n",
    "    # Predict\n",
    "    probabilities = pipeline['model'].predict_proba(processed_data)\n",
    "    churn_probability = probabilities[:, 1]  # probability of class 1 (churn)\n",
    "    \n",
    "    # Apply threshold\n",
    "    predictions = (churn_probability >= pipeline['model_info']['threshold']).astype(int)\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'churn_probabilities': churn_probability,\n",
    "        'no_churn_probabilities': probabilities[:, 0]\n",
    "    }\n",
    "\n",
    "# Load model\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    try:\n",
    "        return joblib.load('churn_complete_model.joblib')\n",
    "    except FileNotFoundError:\n",
    "        st.error(\"Model file not found. Please upload churn_complete_model.joblib to your repository.\")\n",
    "        return None\n",
    "\n",
    "# Main app\n",
    "def main():\n",
    "    st.title(\"🔮 Customer Churn Prediction\")\n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    # Load pipeline\n",
    "    pipeline = load_model()\n",
    "    \n",
    "    if pipeline is None:\n",
    "        st.stop()\n",
    "    \n",
    "    # Create two columns\n",
    "    col1, col2 = st.columns([2, 1])\n",
    "    \n",
    "    with col1:\n",
    "        st.subheader(\"Customer Information\")\n",
    "        \n",
    "        # Create input form\n",
    "        with st.form(\"prediction_form\"):\n",
    "            # Customer demographics\n",
    "            st.write(\"**Demographics**\")\n",
    "            col_demo1, col_demo2 = st.columns(2)\n",
    "            \n",
    "            with col_demo1:\n",
    "                senior_citizen = st.selectbox(\"Senior Citizen\", [\"No\", \"Yes\"])\n",
    "                partner = st.selectbox(\"Partner\", [\"No\", \"Yes\"])\n",
    "                dependents = st.selectbox(\"Dependents\", [\"No\", \"Yes\"])\n",
    "            \n",
    "            with col_demo2:\n",
    "                tenure = st.number_input(\"Tenure (months)\", min_value=0, max_value=100, value=12)\n",
    "                phone_service = st.selectbox(\"Phone Service\", [\"No\", \"Yes\"])\n",
    "                multiple_lines = st.selectbox(\"Multiple Lines\", [\"No\", \"Yes\", \"No phone service\"])\n",
    "            \n",
    "            # Services\n",
    "            st.write(\"**Services**\")\n",
    "            col_serv1, col_serv2 = st.columns(2)\n",
    "            \n",
    "            with col_serv1:\n",
    "                internet_service = st.selectbox(\"Internet Service\", [\"DSL\", \"Fiber optic\", \"No\"])\n",
    "                online_security = st.selectbox(\"Online Security\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                online_backup = st.selectbox(\"Online Backup\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                device_protection = st.selectbox(\"Device Protection\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "            \n",
    "            with col_serv2:\n",
    "                tech_support = st.selectbox(\"Tech Support\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                streaming_tv = st.selectbox(\"Streaming TV\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                streaming_movies = st.selectbox(\"Streaming Movies\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                paperless_billing = st.selectbox(\"Paperless Billing\", [\"No\", \"Yes\"])\n",
    "            \n",
    "            # Contract and payment\n",
    "            st.write(\"**Contract & Payment**\")\n",
    "            col_pay1, col_pay2 = st.columns(2)\n",
    "            \n",
    "            with col_pay1:\n",
    "                contract = st.selectbox(\"Contract\", [\"Month-to-month\", \"One year\", \"Two year\"])\n",
    "                payment_method = st.selectbox(\"Payment Method\", \n",
    "                    [\"Electronic check\", \"Mailed check\", \"Bank transfer (automatic)\", \"Credit card (automatic)\"])\n",
    "            \n",
    "            with col_pay2:\n",
    "                monthly_charges = st.number_input(\"Monthly Charges ($)\", min_value=0.0, max_value=200.0, value=50.0)\n",
    "                total_charges = st.number_input(\"Total Charges ($)\", min_value=0.0, max_value=10000.0, value=500.0)\n",
    "            \n",
    "            # Submit button\n",
    "            submitted = st.form_submit_button(\"🔍 Predict Churn\", use_container_width=True)\n",
    "            \n",
    "            if submitted:\n",
    "                # Create input dataframe\n",
    "                input_data = pd.DataFrame({\n",
    "                    'Customer ID': ['TEST_001'],  # Dummy ID\n",
    "                    'Senior Citizen': [senior_citizen],\n",
    "                    'Partner': [partner],\n",
    "                    'Dependents': [dependents],\n",
    "                    'Tenure': [tenure],\n",
    "                    'Phone Service': [phone_service],\n",
    "                    'Multiple Lines': [multiple_lines],\n",
    "                    'Internet Service': [internet_service],\n",
    "                    'Online Security': [online_security],\n",
    "                    'Online Backup': [online_backup],\n",
    "                    'Device Protection': [device_protection],\n",
    "                    'Tech Support': [tech_support],\n",
    "                    'Streaming TV': [streaming_tv],\n",
    "                    'Streaming Movies': [streaming_movies],\n",
    "                    'Paperless Billing': [paperless_billing],\n",
    "                    'Contract': [contract],\n",
    "                    'Payment Method': [payment_method],\n",
    "                    'Monthly Charges': [monthly_charges],\n",
    "                    'Total Charges': [total_charges]\n",
    "                })\n",
    "                \n",
    "                try:\n",
    "                    # Make prediction using your pipeline\n",
    "                    result = pipeline['predict_function'](input_data, 'churn_complete_model.joblib')\n",
    "                    \n",
    "                    # Display results in the second column\n",
    "                    with col2:\n",
    "                        st.subheader(\"Prediction Results\")\n",
    "                        \n",
    "                        churn_prob = result['churn_probabilities'][0]\n",
    "                        prediction = result['predictions'][0]\n",
    "                        \n",
    "                        # Show probability\n",
    "                        st.metric(\n",
    "                            label=\"Churn Probability\",\n",
    "                            value=f\"{churn_prob:.1%}\",\n",
    "                            delta=f\"{'High Risk' if churn_prob > 0.5 else 'Low Risk'}\"\n",
    "                        )\n",
    "                        \n",
    "                        # Show prediction\n",
    "                        if prediction == 1:\n",
    "                            st.error(\"🚨 **LIKELY TO CHURN**\")\n",
    "                            st.write(\"This customer has a high probability of churning.\")\n",
    "                        else:\n",
    "                            st.success(\"✅ **LIKELY TO STAY**\")\n",
    "                            st.write(\"This customer has a low probability of churning.\")\n",
    "                        \n",
    "                        # Progress bar\n",
    "                        st.write(\"**Risk Level:**\")\n",
    "                        st.progress(churn_prob)\n",
    "                        \n",
    "                        # Additional insights\n",
    "                        st.write(\"**Key Factors:**\")\n",
    "                        if contract == \"Month-to-month\":\n",
    "                            st.write(\"- Month-to-month contract increases churn risk\")\n",
    "                        if tenure < 12:\n",
    "                            st.write(\"- Low tenure increases churn risk\")\n",
    "                        if monthly_charges > 70:\n",
    "                            st.write(\"- High monthly charges increase churn risk\")\n",
    "                        if internet_service == \"Fiber optic\":\n",
    "                            st.write(\"- Fiber optic service may increase churn risk\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    st.error(f\"Prediction error: {str(e)}\")\n",
    "                    st.write(\"Please check your model pipeline and input data format.\")\n",
    "    \n",
    "    # Footer\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        <div style='text-align: center; color: #666666;'>\n",
    "            <p>Built with Streamlit | Customer Churn Prediction Model</p>\n",
    "        </div>\n",
    "        \"\"\", \n",
    "        unsafe_allow_html=True\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save app.py\n",
    "with open('app.py', 'w') as f:\n",
    "    f.write(app)\n",
    "print(\"app.py created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2683a8b-a592-439b-b537-4967f00b93f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W\n"
     ]
    }
   ],
   "source": [
    "print('W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0163f21-5d7b-4985-befb-ec3f7189f4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app.py created\n"
     ]
    }
   ],
   "source": [
    "debuging_app = '''import streamlit as st\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "\n",
    "# Page config\n",
    "st.set_page_config(\n",
    "    page_title=\"Churn Prediction App\",\n",
    "    page_icon=\"📊\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "def preprocess_new_data(new_data, preprocessing_info):\n",
    "    \"\"\"\n",
    "    Preprocess new data the same way as train data\n",
    "    \"\"\"\n",
    "    df = new_data.copy()\n",
    "    \n",
    "    # Remove ID cols\n",
    "    df = df.drop(columns=[col for col in preprocessing_info['id_cols'] if col in df.columns], errors='ignore')\n",
    "    \n",
    "    # Label encoding binary columns - FIX: Use consistent LabelEncoder\n",
    "    for col in preprocessing_info['bin_cols']:\n",
    "        if col in df.columns and col != 'Churn':  # exclude target if present\n",
    "            # Use saved encoder if available, otherwise create new one\n",
    "            if 'encoders' in preprocessing_info and col in preprocessing_info['encoders']:\n",
    "                le = preprocessing_info['encoders'][col]\n",
    "                try:\n",
    "                    df[col] = le.transform(df[col].astype(str))\n",
    "                except ValueError:\n",
    "                    # If value not seen in training, use most common value\n",
    "                    df[col] = le.transform([le.classes_[0]] * len(df))[0]\n",
    "            else:\n",
    "                # Simple mapping for binary columns\n",
    "                df[col] = df[col].map({'No': 0, 'Yes': 1}).fillna(0)\n",
    "    \n",
    "    # get_dummies for multi-values columns\n",
    "    df = pd.get_dummies(data=df, columns=preprocessing_info['multi_cols'])\n",
    "    \n",
    "    # Make sure we have all dummy columns (add missing ones with 0 values)\n",
    "    for col in preprocessing_info['final_feature_names']:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "    \n",
    "    # Scaling numerical columns\n",
    "    if preprocessing_info['num_cols']:\n",
    "        scaled_nums = preprocessing_info['scaler'].transform(df[preprocessing_info['num_cols']])\n",
    "        scaled_df = pd.DataFrame(scaled_nums, columns=preprocessing_info['num_cols'], index=df.index)\n",
    "        \n",
    "        # Replace original numerical columns with scaled ones\n",
    "        df = df.drop(columns=preprocessing_info['num_cols'])\n",
    "        df = df.merge(scaled_df, left_index=True, right_index=True, how=\"left\")\n",
    "    \n",
    "    # Ensure we have all required columns in correct order\n",
    "    df = df.reindex(columns=preprocessing_info['final_feature_names'], fill_value=0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def predict_churn(input_data, pipeline_path='churn_model_pipeline.joblib'):\n",
    "    \"\"\"\n",
    "    Complete predict function - from raw data to result\n",
    "    \"\"\"\n",
    "    # Load pipeline\n",
    "    pipeline = joblib.load(pipeline_path)\n",
    "    \n",
    "    # Preprocess data\n",
    "    processed_data = preprocess_new_data(input_data, pipeline['preprocessing_info'])\n",
    "    \n",
    "    # Predict\n",
    "    probabilities = pipeline['model'].predict_proba(processed_data)\n",
    "    churn_probability = probabilities[:, 1]  # probability of class 1 (churn)\n",
    "    \n",
    "    # Apply threshold\n",
    "    predictions = (churn_probability >= pipeline['model_info']['threshold']).astype(int)\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'churn_probabilities': churn_probability,\n",
    "        'no_churn_probabilities': probabilities[:, 0]\n",
    "    }\n",
    "\n",
    "# Load model\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    try:\n",
    "        return joblib.load('churn_complete_model.joblib')\n",
    "    except FileNotFoundError:\n",
    "        st.error(\"Model file not found. Please upload churn_complete_model.joblib to your repository.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Main app\n",
    "def main():\n",
    "    st.title(\"🔮 Customer Churn Prediction\")\n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    # Load pipeline\n",
    "    pipeline = load_model()\n",
    "    \n",
    "    if pipeline is None:\n",
    "        st.stop()\n",
    "    \n",
    "    # Debug: Show pipeline structure\n",
    "    with st.expander(\"Debug: Pipeline Info\"):\n",
    "        st.write(\"Pipeline keys:\", list(pipeline.keys()) if isinstance(pipeline, dict) else \"Not a dict\")\n",
    "        st.write(\"Pipeline type:\", type(pipeline))\n",
    "    \n",
    "    # Create two columns\n",
    "    col1, col2 = st.columns([2, 1])\n",
    "    \n",
    "    with col1:\n",
    "        st.subheader(\"Customer Information\")\n",
    "        \n",
    "        # Create input form\n",
    "        with st.form(\"prediction_form\"):\n",
    "            # Customer demographics\n",
    "            st.write(\"**Demographics**\")\n",
    "            col_demo1, col_demo2 = st.columns(2)\n",
    "            \n",
    "            with col_demo1:\n",
    "                senior_citizen = st.selectbox(\"Senior Citizen\", [\"No\", \"Yes\"])\n",
    "                partner = st.selectbox(\"Partner\", [\"No\", \"Yes\"])\n",
    "                dependents = st.selectbox(\"Dependents\", [\"No\", \"Yes\"])\n",
    "            \n",
    "            with col_demo2:\n",
    "                tenure = st.number_input(\"Tenure (months)\", min_value=0, max_value=100, value=12)\n",
    "                phone_service = st.selectbox(\"Phone Service\", [\"No\", \"Yes\"])\n",
    "                multiple_lines = st.selectbox(\"Multiple Lines\", [\"No\", \"Yes\", \"No phone service\"])\n",
    "            \n",
    "            # Services\n",
    "            st.write(\"**Services**\")\n",
    "            col_serv1, col_serv2 = st.columns(2)\n",
    "            \n",
    "            with col_serv1:\n",
    "                internet_service = st.selectbox(\"Internet Service\", [\"DSL\", \"Fiber optic\", \"No\"])\n",
    "                online_security = st.selectbox(\"Online Security\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                online_backup = st.selectbox(\"Online Backup\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                device_protection = st.selectbox(\"Device Protection\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "            \n",
    "            with col_serv2:\n",
    "                tech_support = st.selectbox(\"Tech Support\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                streaming_tv = st.selectbox(\"Streaming TV\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                streaming_movies = st.selectbox(\"Streaming Movies\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                paperless_billing = st.selectbox(\"Paperless Billing\", [\"No\", \"Yes\"])\n",
    "            \n",
    "            # Contract and payment\n",
    "            st.write(\"**Contract & Payment**\")\n",
    "            col_pay1, col_pay2 = st.columns(2)\n",
    "            \n",
    "            with col_pay1:\n",
    "                contract = st.selectbox(\"Contract\", [\"Month-to-month\", \"One year\", \"Two year\"])\n",
    "                payment_method = st.selectbox(\"Payment Method\", \n",
    "                    [\"Electronic check\", \"Mailed check\", \"Bank transfer (automatic)\", \"Credit card (automatic)\"])\n",
    "            \n",
    "            with col_pay2:\n",
    "                monthly_charges = st.number_input(\"Monthly Charges ($)\", min_value=0.0, max_value=500.0, value=50.0)\n",
    "                total_charges = st.number_input(\"Total Charges ($)\", min_value=0.0, max_value=50000.0, value=500.0)\n",
    "            \n",
    "            # Submit button\n",
    "            submitted = st.form_submit_button(\"🔍 Predict Churn\", use_container_width=True)\n",
    "            \n",
    "            if submitted:\n",
    "                # Create input dataframe\n",
    "                input_data = pd.DataFrame({\n",
    "                    'Customer ID': ['TEST_001'],  # Dummy ID\n",
    "                    'Senior Citizen': [senior_citizen],\n",
    "                    'Partner': [partner],\n",
    "                    'Dependents': [dependents],\n",
    "                    'Tenure': [tenure],\n",
    "                    'Phone Service': [phone_service],\n",
    "                    'Multiple Lines': [multiple_lines],\n",
    "                    'Internet Service': [internet_service],\n",
    "                    'Online Security': [online_security],\n",
    "                    'Online Backup': [online_backup],\n",
    "                    'Device Protection': [device_protection],\n",
    "                    'Tech Support': [tech_support],\n",
    "                    'Streaming TV': [streaming_tv],\n",
    "                    'Streaming Movies': [streaming_movies],\n",
    "                    'Paperless Billing': [paperless_billing],\n",
    "                    'Contract': [contract],\n",
    "                    'Payment Method': [payment_method],\n",
    "                    'Monthly Charges': [monthly_charges],\n",
    "                    'Total Charges': [total_charges]\n",
    "                })\n",
    "                \n",
    "                try:\n",
    "                    # Check if pipeline is a dict with specific structure\n",
    "                    if isinstance(pipeline, dict) and 'predict_function' in pipeline:\n",
    "                        # Use your custom pipeline structure\n",
    "                        result = pipeline['predict_function'](input_data, 'churn_complete_model.joblib')\n",
    "                        churn_prob = result['churn_probabilities'][0]\n",
    "                        prediction = result['predictions'][0]\n",
    "                        \n",
    "                    elif isinstance(pipeline, dict) and 'model' in pipeline:\n",
    "                        # Use dictionary structure\n",
    "                        processed_data = preprocess_new_data(input_data, pipeline['preprocessing_info'])\n",
    "                        probabilities = pipeline['model'].predict_proba(processed_data)\n",
    "                        churn_prob = probabilities[0, 1]\n",
    "                        prediction = 1 if churn_prob > 0.5 else 0\n",
    "                        \n",
    "                    else:\n",
    "                        # Assume it's a scikit-learn pipeline\n",
    "                        probabilities = pipeline.predict_proba(input_data)\n",
    "                        churn_prob = probabilities[0, 1]\n",
    "                        prediction = 1 if churn_prob > 0.5 else 0\n",
    "                    \n",
    "                    # Display results in the second column\n",
    "                    with col2:\n",
    "                        st.subheader(\"Prediction Results\")\n",
    "                        \n",
    "                        # Debug info\n",
    "                        st.write(f\"Debug: Churn probability = {churn_prob:.4f}\")\n",
    "                        st.write(f\"Debug: Prediction = {prediction}\")\n",
    "                        \n",
    "                        # Additional debug for the scaling issue\n",
    "                        if isinstance(pipeline, dict) and 'preprocessing_info' in pipeline:\n",
    "                            st.write(f\"Debug: Monthly charges input = {monthly_charges}\")\n",
    "                            st.write(f\"Debug: Total charges input = {total_charges}\")\n",
    "                            \n",
    "                            # Show processed data\n",
    "                            processed_data = preprocess_new_data(input_data, pipeline['preprocessing_info'])\n",
    "                            st.write(\"Debug: Processed data shape:\", processed_data.shape)\n",
    "                            \n",
    "                            # Show specific columns if they exist\n",
    "                            if 'Monthly Charges' in processed_data.columns:\n",
    "                                st.write(f\"Debug: Processed Monthly Charges = {processed_data['Monthly Charges'].iloc[0]:.4f}\")\n",
    "                            if 'Total Charges' in processed_data.columns:\n",
    "                                st.write(f\"Debug: Processed Total Charges = {processed_data['Total Charges'].iloc[0]:.4f}\")\n",
    "                                \n",
    "                            # Show first few processed features\n",
    "                            st.write(\"Debug: First 10 processed features:\")\n",
    "                            st.write(processed_data.iloc[0, :10].to_dict())\n",
    "                        \n",
    "                        # Show probability\n",
    "                        st.metric(\n",
    "                            label=\"Churn Probability\",\n",
    "                            value=f\"{churn_prob:.1%}\",\n",
    "                            delta=f\"{'High Risk' if churn_prob > 0.5 else 'Low Risk'}\"\n",
    "                        )\n",
    "                        \n",
    "                        # Show prediction\n",
    "                        if prediction == 1:\n",
    "                            st.error(\"🚨 **LIKELY TO CHURN**\")\n",
    "                            st.write(\"This customer has a high probability of churning.\")\n",
    "                        else:\n",
    "                            st.success(\"✅ **LIKELY TO STAY**\")\n",
    "                            st.write(\"This customer has a low probability of churning.\")\n",
    "                        \n",
    "                        # Progress bar\n",
    "                        st.write(\"**Risk Level:**\")\n",
    "                        st.progress(churn_prob)\n",
    "                        \n",
    "                        # Additional insights\n",
    "                        st.write(\"**Key Factors:**\")\n",
    "                        if contract == \"Month-to-month\":\n",
    "                            st.write(\"- Month-to-month contract increases churn risk\")\n",
    "                        if tenure < 12:\n",
    "                            st.write(\"- Low tenure increases churn risk\")\n",
    "                        if monthly_charges > 70:\n",
    "                            st.write(\"- High monthly charges increase churn risk\")\n",
    "                        if internet_service == \"Fiber optic\":\n",
    "                            st.write(\"- Fiber optic service may increase churn risk\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    st.error(f\"Prediction error: {str(e)}\")\n",
    "                    st.write(\"Please check your model pipeline and input data format.\")\n",
    "                    \n",
    "                    # Debug information\n",
    "                    with st.expander(\"Debug Information\"):\n",
    "                        st.write(\"Input data shape:\", input_data.shape)\n",
    "                        st.write(\"Input data columns:\", list(input_data.columns))\n",
    "                        st.write(\"Input data preview:\")\n",
    "                        st.write(input_data)\n",
    "                        st.write(\"Error details:\", str(e))\n",
    "    \n",
    "    # Footer\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        <div style='text-align: center; color: #666666;'>\n",
    "            <p>Built with Streamlit | Customer Churn Prediction Model</p>\n",
    "        </div>\n",
    "        \"\"\", \n",
    "        unsafe_allow_html=True\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save app.py\n",
    "with open('app.py', 'w') as f:\n",
    "    f.write(debuging_app)\n",
    "print(\"app.py created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beecee64-a5ac-475b-a2c8-2c8d68b1d124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app.py created\n"
     ]
    }
   ],
   "source": [
    "debuging_app2 = '''import streamlit as st\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "\n",
    "# Page config\n",
    "st.set_page_config(\n",
    "    page_title=\"Churn Prediction App\",\n",
    "    page_icon=\"📊\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "def preprocess_new_data(new_data, preprocessing_info):\n",
    "    \"\"\"\n",
    "    Preprocess new data the same way as train data - FIXED VERSION\n",
    "    \"\"\"\n",
    "    df = new_data.copy()\n",
    "    \n",
    "    # Remove ID cols\n",
    "    df = df.drop(columns=[col for col in preprocessing_info['id_cols'] if col in df.columns], errors='ignore')\n",
    "    \n",
    "    # FIXED: Apply scaling BEFORE get_dummies (same order as training)\n",
    "    if preprocessing_info['num_cols']:\n",
    "        # Apply scaling to ALL numerical columns at once (same as training)\n",
    "        num_cols_present = [col for col in preprocessing_info['num_cols'] if col in df.columns]\n",
    "        if num_cols_present:\n",
    "            scaled_values = preprocessing_info['scaler'].transform(df[num_cols_present])\n",
    "            df[num_cols_present] = scaled_values\n",
    "    \n",
    "    # FIXED: Use saved label encoders instead of fitting new ones\n",
    "    for col in preprocessing_info['bin_cols']:\n",
    "        if col in df.columns and col != 'Churn':  # exclude target if present\n",
    "            if col in preprocessing_info['label_encoders']:\n",
    "                # Use the saved encoder from training\n",
    "                df[col] = preprocessing_info['label_encoders'][col].transform(df[col].astype(str))\n",
    "            else:\n",
    "                # Fallback - create mapping manually\n",
    "                df[col] = df[col].map({'No': 0, 'Yes': 1}).fillna(0)\n",
    "    \n",
    "    # get_dummies for multi-values columns (after scaling)\n",
    "    df = pd.get_dummies(data=df, columns=preprocessing_info['multi_cols'])\n",
    "    \n",
    "    # Make sure we have all dummy columns (add missing ones with 0 values)\n",
    "    for col in preprocessing_info['final_feature_names']:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "    \n",
    "    # Ensure correct column order\n",
    "    df = df[preprocessing_info['final_feature_names']]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def predict_churn(input_data, pipeline_path='churn_model_pipeline.joblib'):\n",
    "    \"\"\"\n",
    "    Complete predict function - from raw data to result\n",
    "    \"\"\"\n",
    "    # Load pipeline\n",
    "    pipeline = joblib.load(pipeline_path)\n",
    "    \n",
    "    # Preprocess data\n",
    "    processed_data = preprocess_new_data(input_data, pipeline['preprocessing_info'])\n",
    "    \n",
    "    # Predict\n",
    "    probabilities = pipeline['model'].predict_proba(processed_data)\n",
    "    churn_probability = probabilities[:, 1]  # probability of class 1 (churn)\n",
    "    \n",
    "    # Apply threshold\n",
    "    predictions = (churn_probability >= pipeline['model_info']['threshold']).astype(int)\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'churn_probabilities': churn_probability,\n",
    "        'no_churn_probabilities': probabilities[:, 0]\n",
    "    }\n",
    "\n",
    "# Load model\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    try:\n",
    "        return joblib.load('churn_complete_model.joblib')\n",
    "    except FileNotFoundError:\n",
    "        st.error(\"Model file not found. Please upload churn_complete_model.joblib to your repository.\")\n",
    "        return None\n",
    "\n",
    "# ADDED: Debug function to check preprocessing\n",
    "def debug_preprocessing(input_data, pipeline):\n",
    "    \"\"\"Debug function to check if preprocessing is working correctly\"\"\"\n",
    "    st.write(\"**Debug Information:**\")\n",
    "    \n",
    "    # Show original input\n",
    "    st.write(\"Original input columns:\")\n",
    "    st.write(list(input_data.columns))\n",
    "    \n",
    "    # Show preprocessing info\n",
    "    st.write(\"Expected numerical columns:\")\n",
    "    st.write(pipeline['preprocessing_info']['num_cols'])\n",
    "    \n",
    "    st.write(\"Expected binary columns:\")\n",
    "    st.write(pipeline['preprocessing_info']['bin_cols'])\n",
    "    \n",
    "    st.write(\"Expected multi-category columns:\")\n",
    "    st.write(pipeline['preprocessing_info']['multi_cols'])\n",
    "    \n",
    "    st.write(\"Expected final feature names (first 10):\")\n",
    "    st.write(pipeline['preprocessing_info']['final_feature_names'][:10])\n",
    "    \n",
    "    # Show preprocessing steps\n",
    "    try:\n",
    "        # Step by step preprocessing\n",
    "        df = input_data.copy()\n",
    "        st.write(f\"1. After copy: {df.shape}\")\n",
    "        \n",
    "        # Remove ID cols\n",
    "        df = df.drop(columns=[col for col in pipeline['preprocessing_info']['id_cols'] if col in df.columns], errors='ignore')\n",
    "        st.write(f\"2. After removing ID cols: {df.shape}\")\n",
    "        \n",
    "        # Scale numerical columns\n",
    "        if pipeline['preprocessing_info']['num_cols']:\n",
    "            num_cols_present = [col for col in pipeline['preprocessing_info']['num_cols'] if col in df.columns]\n",
    "            st.write(f\"3. Numerical columns present: {num_cols_present}\")\n",
    "            if num_cols_present:\n",
    "                scaled_values = pipeline['preprocessing_info']['scaler'].transform(df[num_cols_present])\n",
    "                df[num_cols_present] = scaled_values\n",
    "                st.write(f\"4. After scaling: {df.shape}\")\n",
    "        \n",
    "        # Binary encoding\n",
    "        for col in pipeline['preprocessing_info']['bin_cols']:\n",
    "            if col in df.columns and col != 'Churn':\n",
    "                if col in pipeline['preprocessing_info']['label_encoders']:\n",
    "                    df[col] = pipeline['preprocessing_info']['label_encoders'][col].transform(df[col].astype(str))\n",
    "                else:\n",
    "                    df[col] = df[col].map({'No': 0, 'Yes': 1}).fillna(0)\n",
    "        st.write(f\"5. After binary encoding: {df.shape}\")\n",
    "        \n",
    "        # Get dummies\n",
    "        df = pd.get_dummies(data=df, columns=pipeline['preprocessing_info']['multi_cols'])\n",
    "        st.write(f\"6. After get_dummies: {df.shape}\")\n",
    "        st.write(f\"   Columns after get_dummies: {list(df.columns)}\")\n",
    "        \n",
    "        # Add missing columns\n",
    "        missing_cols = []\n",
    "        for col in pipeline['preprocessing_info']['final_feature_names']:\n",
    "            if col not in df.columns:\n",
    "                df[col] = 0\n",
    "                missing_cols.append(col)\n",
    "        \n",
    "        if missing_cols:\n",
    "            st.write(f\"7. Added missing columns: {len(missing_cols)}\")\n",
    "            st.write(f\"   Missing columns: {missing_cols[:5]}...\")  # Show first 5\n",
    "        \n",
    "        # Final ordering\n",
    "        df = df[pipeline['preprocessing_info']['final_feature_names']]\n",
    "        st.write(f\"8. Final shape: {df.shape}\")\n",
    "        \n",
    "        st.write(\"Preprocessing successful!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        st.error(f\"Preprocessing error: {str(e)}\")\n",
    "        st.exception(e)\n",
    "\n",
    "# Main app\n",
    "def main():\n",
    "    st.title(\"🔮 Customer Churn Prediction\")\n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    # Load pipeline\n",
    "    pipeline = load_model()\n",
    "    \n",
    "    if pipeline is None:\n",
    "        st.stop()\n",
    "    \n",
    "    # ADDED: Debug toggle\n",
    "    debug_mode = st.sidebar.checkbox(\"Debug Mode\", value=False)\n",
    "    \n",
    "    # Create two columns\n",
    "    col1, col2 = st.columns([2, 1])\n",
    "    \n",
    "    with col1:\n",
    "        st.subheader(\"Customer Information\")\n",
    "        \n",
    "        # Create input form\n",
    "        with st.form(\"prediction_form\"):\n",
    "            # Customer demographics\n",
    "            st.write(\"**Demographics**\")\n",
    "            col_demo1, col_demo2 = st.columns(2)\n",
    "            \n",
    "            with col_demo1:\n",
    "                senior_citizen = st.selectbox(\"Senior Citizen\", [\"No\", \"Yes\"])\n",
    "                partner = st.selectbox(\"Partner\", [\"No\", \"Yes\"])\n",
    "                dependents = st.selectbox(\"Dependents\", [\"No\", \"Yes\"])\n",
    "            \n",
    "            with col_demo2:\n",
    "                # FIXED: Set more realistic default values\n",
    "                tenure = st.number_input(\"Tenure (months)\", min_value=0, max_value=100, value=12)\n",
    "                phone_service = st.selectbox(\"Phone Service\", [\"No\", \"Yes\"])\n",
    "                multiple_lines = st.selectbox(\"Multiple Lines\", [\"No\", \"Yes\", \"No phone service\"])\n",
    "            \n",
    "            # Services\n",
    "            st.write(\"**Services**\")\n",
    "            col_serv1, col_serv2 = st.columns(2)\n",
    "            \n",
    "            with col_serv1:\n",
    "                internet_service = st.selectbox(\"Internet Service\", [\"DSL\", \"Fiber optic\", \"No\"])\n",
    "                online_security = st.selectbox(\"Online Security\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                online_backup = st.selectbox(\"Online Backup\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                device_protection = st.selectbox(\"Device Protection\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "            \n",
    "            with col_serv2:\n",
    "                tech_support = st.selectbox(\"Tech Support\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                streaming_tv = st.selectbox(\"Streaming TV\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                streaming_movies = st.selectbox(\"Streaming Movies\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                paperless_billing = st.selectbox(\"Paperless Billing\", [\"No\", \"Yes\"])\n",
    "            \n",
    "            # Contract and payment\n",
    "            st.write(\"**Contract & Payment**\")\n",
    "            col_pay1, col_pay2 = st.columns(2)\n",
    "            \n",
    "            with col_pay1:\n",
    "                contract = st.selectbox(\"Contract\", [\"Month-to-month\", \"One year\", \"Two year\"])\n",
    "                payment_method = st.selectbox(\"Payment Method\", \n",
    "                    [\"Electronic check\", \"Mailed check\", \"Bank transfer (automatic)\", \"Credit card (automatic)\"])\n",
    "            \n",
    "            with col_pay2:\n",
    "                # FIXED: Set ranges based on your training data\n",
    "                monthly_charges = st.number_input(\"Monthly Charges ($)\", \n",
    "                    min_value=18.0, max_value=120.0, value=50.0, step=0.25)\n",
    "                total_charges = st.number_input(\"Total Charges ($)\", \n",
    "                    min_value=18.0, max_value=9000.0, value=500.0, step=0.1)\n",
    "            \n",
    "            # Submit button\n",
    "            submitted = st.form_submit_button(\"🔍 Predict Churn\", use_container_width=True)\n",
    "            \n",
    "            if submitted:\n",
    "                # Create input dataframe\n",
    "                input_data = pd.DataFrame({\n",
    "                    'Customer ID': ['TEST_001'],  # Dummy ID\n",
    "                    'Senior Citizen': [senior_citizen],\n",
    "                    'Partner': [partner],\n",
    "                    'Dependents': [dependents],\n",
    "                    'Tenure': [tenure],\n",
    "                    'Phone Service': [phone_service],\n",
    "                    'Multiple Lines': [multiple_lines],\n",
    "                    'Internet Service': [internet_service],\n",
    "                    'Online Security': [online_security],\n",
    "                    'Online Backup': [online_backup],\n",
    "                    'Device Protection': [device_protection],\n",
    "                    'Tech Support': [tech_support],\n",
    "                    'Streaming TV': [streaming_tv],\n",
    "                    'Streaming Movies': [streaming_movies],\n",
    "                    'Paperless Billing': [paperless_billing],\n",
    "                    'Contract': [contract],\n",
    "                    'Payment Method': [payment_method],\n",
    "                    'Monthly Charges': [monthly_charges],\n",
    "                    'Total Charges': [total_charges]\n",
    "                })\n",
    "                \n",
    "                # ADDED: Debug preprocessing if enabled\n",
    "                if debug_mode:\n",
    "                    debug_preprocessing(input_data, pipeline)\n",
    "                \n",
    "                try:\n",
    "                    # Make prediction using your pipeline\n",
    "                    result = pipeline['predict_function'](input_data, 'churn_complete_model.joblib')\n",
    "                    \n",
    "                    # Display results in the second column\n",
    "                    with col2:\n",
    "                        st.subheader(\"Prediction Results\")\n",
    "                        \n",
    "                        churn_prob = result['churn_probabilities'][0]\n",
    "                        prediction = result['predictions'][0]\n",
    "                        \n",
    "                        # Show probability\n",
    "                        st.metric(\n",
    "                            label=\"Churn Probability\",\n",
    "                            value=f\"{churn_prob:.1%}\",\n",
    "                            delta=f\"{'High Risk' if churn_prob > 0.5 else 'Low Risk'}\"\n",
    "                        )\n",
    "                        \n",
    "                        # Show prediction\n",
    "                        if prediction == 1:\n",
    "                            st.error(\"🚨 **LIKELY TO CHURN**\")\n",
    "                            st.write(\"This customer has a high probability of churning.\")\n",
    "                        else:\n",
    "                            st.success(\"✅ **LIKELY TO STAY**\")\n",
    "                            st.write(\"This customer has a low probability of churning.\")\n",
    "                        \n",
    "                        # Progress bar\n",
    "                        st.write(\"**Risk Level:**\")\n",
    "                        st.progress(churn_prob)\n",
    "                        \n",
    "                        # IMPROVED: More logical insights\n",
    "                        st.write(\"**Key Risk Factors:**\")\n",
    "                        risk_factors = []\n",
    "                        \n",
    "                        if contract == \"Month-to-month\":\n",
    "                            risk_factors.append(\"Month-to-month contract increases churn risk\")\n",
    "                        if tenure < 12:\n",
    "                            risk_factors.append(\"Low tenure increases churn risk\")\n",
    "                        if monthly_charges > 70:\n",
    "                            risk_factors.append(\"High monthly charges increase churn risk\")\n",
    "                        if internet_service == \"Fiber optic\":\n",
    "                            risk_factors.append(\"Fiber optic service may increase churn risk\")\n",
    "                        if payment_method == \"Electronic check\":\n",
    "                            risk_factors.append(\"Electronic check payment increases churn risk\")\n",
    "                        if paperless_billing == \"Yes\":\n",
    "                            risk_factors.append(\"Paperless billing may increase churn risk\")\n",
    "                        \n",
    "                        if risk_factors:\n",
    "                            for factor in risk_factors:\n",
    "                                st.write(f\"- {factor}\")\n",
    "                        else:\n",
    "                            st.write(\"- No major risk factors identified\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    st.error(f\"Prediction error: {str(e)}\")\n",
    "                    st.write(\"Please check your model pipeline and input data format.\")\n",
    "                    \n",
    "                    # ADDED: Show more detailed error info\n",
    "                    if debug_mode:\n",
    "                        st.write(\"**Error details:**\")\n",
    "                        st.exception(e)\n",
    "    \n",
    "    # Footer\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        <div style='text-align: center; color: #666666;'>\n",
    "            <p>Built with Streamlit | Customer Churn Prediction Model</p>\n",
    "        </div>\n",
    "        \"\"\", \n",
    "        unsafe_allow_html=True\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "# Save app.py\n",
    "with open('app.py', 'w') as f:\n",
    "    f.write(debuging_app2)\n",
    "print(\"app.py created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00024c15-7f41-4bd9-bbe6-ccd033a00091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app.py created\n"
     ]
    }
   ],
   "source": [
    "debuging_app3 = '''import streamlit as st\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "\n",
    "# Page config\n",
    "st.set_page_config(\n",
    "    page_title=\"Churn Prediction App\",\n",
    "    page_icon=\"📊\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "def preprocess_new_data(new_data, preprocessing_info):\n",
    "    \"\"\"\n",
    "    Preprocess new data the same way as train data - IMPROVED VERSION\n",
    "    \"\"\"\n",
    "    df = new_data.copy()\n",
    "    \n",
    "    # Remove ID cols\n",
    "    df = df.drop(columns=[col for col in preprocessing_info['id_cols'] if col in df.columns], errors='ignore')\n",
    "    \n",
    "    # CRITICAL FIX: Apply label encoding for binary columns FIRST\n",
    "    for col in preprocessing_info['bin_cols']:\n",
    "        if col in df.columns and col != 'Churn':  # exclude target if present\n",
    "            if col in preprocessing_info['label_encoders']:\n",
    "                # Use the saved encoder from training\n",
    "                try:\n",
    "                    df[col] = preprocessing_info['label_encoders'][col].transform(df[col].astype(str))\n",
    "                except ValueError as e:\n",
    "                    st.warning(f\"Unknown category in {col}: {df[col].unique()}\")\n",
    "                    # Handle unknown categories by mapping to most frequent class\n",
    "                    df[col] = df[col].map({'No': 0, 'Yes': 1}).fillna(0)\n",
    "            else:\n",
    "                # Fallback - create mapping manually\n",
    "                df[col] = df[col].map({'No': 0, 'Yes': 1}).fillna(0)\n",
    "    \n",
    "    # Apply get_dummies for multi-category columns\n",
    "    df = pd.get_dummies(data=df, columns=preprocessing_info['multi_cols'], drop_first=False)\n",
    "    \n",
    "    # CRITICAL FIX: Apply scaling AFTER categorical encoding\n",
    "    if preprocessing_info['num_cols']:\n",
    "        # Apply scaling to ALL numerical columns at once (same as training)\n",
    "        num_cols_present = [col for col in preprocessing_info['num_cols'] if col in df.columns]\n",
    "        if num_cols_present:\n",
    "            # Make sure we scale only the numerical columns that still exist\n",
    "            scaled_values = preprocessing_info['scaler'].transform(df[num_cols_present])\n",
    "            df[num_cols_present] = scaled_values\n",
    "    \n",
    "    # Make sure we have all dummy columns (add missing ones with 0 values)\n",
    "    for col in preprocessing_info['final_feature_names']:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "    \n",
    "    # Ensure correct column order\n",
    "    df = df[preprocessing_info['final_feature_names']]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def predict_churn(input_data, pipeline_path='churn_model_pipeline.joblib'):\n",
    "    \"\"\"\n",
    "    Complete predict function - from raw data to result\n",
    "    \"\"\"\n",
    "    # Load pipeline\n",
    "    pipeline = joblib.load(pipeline_path)\n",
    "    \n",
    "    # Preprocess data\n",
    "    processed_data = preprocess_new_data(input_data, pipeline['preprocessing_info'])\n",
    "    \n",
    "    # Predict\n",
    "    probabilities = pipeline['model'].predict_proba(processed_data)\n",
    "    churn_probability = probabilities[:, 1]  # probability of class 1 (churn)\n",
    "    \n",
    "    # Apply threshold\n",
    "    predictions = (churn_probability >= pipeline['model_info']['threshold']).astype(int)\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'churn_probabilities': churn_probability,\n",
    "        'no_churn_probabilities': probabilities[:, 0]\n",
    "    }\n",
    "\n",
    "# Load model\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    try:\n",
    "        return joblib.load('churn_complete_model.joblib')\n",
    "    except FileNotFoundError:\n",
    "        st.error(\"Model file not found. Please upload churn_complete_model.joblib to your repository.\")\n",
    "        return None\n",
    "\n",
    "# IMPROVED: Enhanced debug function with feature importance\n",
    "def debug_preprocessing(input_data, pipeline):\n",
    "    \"\"\"Enhanced debug function to check preprocessing and feature impact\"\"\"\n",
    "    st.write(\"**Debug Information:**\")\n",
    "    \n",
    "    # Show original input\n",
    "    st.write(\"Original input columns:\")\n",
    "    st.write(list(input_data.columns))\n",
    "    \n",
    "    # Show preprocessing info\n",
    "    st.write(\"Expected numerical columns:\")\n",
    "    st.write(pipeline['preprocessing_info']['num_cols'])\n",
    "    \n",
    "    st.write(\"Expected binary columns:\")\n",
    "    st.write(pipeline['preprocessing_info']['bin_cols'])\n",
    "    \n",
    "    st.write(\"Expected multi-category columns:\")\n",
    "    st.write(pipeline['preprocessing_info']['multi_cols'])\n",
    "    \n",
    "    # Show preprocessing steps\n",
    "    try:\n",
    "        # Step by step preprocessing\n",
    "        df = input_data.copy()\n",
    "        st.write(f\"1. After copy: {df.shape}\")\n",
    "        \n",
    "        # Remove ID cols\n",
    "        df = df.drop(columns=[col for col in pipeline['preprocessing_info']['id_cols'] if col in df.columns], errors='ignore')\n",
    "        st.write(f\"2. After removing ID cols: {df.shape}\")\n",
    "        \n",
    "        # Binary encoding FIRST\n",
    "        for col in pipeline['preprocessing_info']['bin_cols']:\n",
    "            if col in df.columns and col != 'Churn':\n",
    "                if col in pipeline['preprocessing_info']['label_encoders']:\n",
    "                    df[col] = pipeline['preprocessing_info']['label_encoders'][col].transform(df[col].astype(str))\n",
    "                else:\n",
    "                    df[col] = df[col].map({'No': 0, 'Yes': 1}).fillna(0)\n",
    "        st.write(f\"3. After binary encoding: {df.shape}\")\n",
    "        \n",
    "        # Get dummies\n",
    "        df = pd.get_dummies(data=df, columns=pipeline['preprocessing_info']['multi_cols'], drop_first=False)\n",
    "        st.write(f\"4. After get_dummies: {df.shape}\")\n",
    "        st.write(f\"   Columns after get_dummies: {list(df.columns)}\")\n",
    "        \n",
    "        # Scale numerical columns AFTER categorical\n",
    "        if pipeline['preprocessing_info']['num_cols']:\n",
    "            num_cols_present = [col for col in pipeline['preprocessing_info']['num_cols'] if col in df.columns]\n",
    "            st.write(f\"5. Numerical columns present: {num_cols_present}\")\n",
    "            if num_cols_present:\n",
    "                st.write(f\"   Values before scaling: {df[num_cols_present].iloc[0].to_dict()}\")\n",
    "                scaled_values = pipeline['preprocessing_info']['scaler'].transform(df[num_cols_present])\n",
    "                df[num_cols_present] = scaled_values\n",
    "                st.write(f\"   Values after scaling: {df[num_cols_present].iloc[0].to_dict()}\")\n",
    "                st.write(f\"6. After scaling: {df.shape}\")\n",
    "        \n",
    "        # Add missing columns\n",
    "        missing_cols = []\n",
    "        for col in pipeline['preprocessing_info']['final_feature_names']:\n",
    "            if col not in df.columns:\n",
    "                df[col] = 0\n",
    "                missing_cols.append(col)\n",
    "        \n",
    "        if missing_cols:\n",
    "            st.write(f\"7. Added missing columns: {len(missing_cols)}\")\n",
    "            st.write(f\"   Missing columns: {missing_cols[:10]}...\")  # Show first 10\n",
    "        \n",
    "        # Final ordering\n",
    "        df = df[pipeline['preprocessing_info']['final_feature_names']]\n",
    "        st.write(f\"8. Final shape: {df.shape}\")\n",
    "        \n",
    "        # NEW: Show final processed values\n",
    "        st.write(\"**Final processed values (first 10 features):**\")\n",
    "        final_values = df.iloc[0].to_dict()\n",
    "        for i, (col, val) in enumerate(list(final_values.items())[:10]):\n",
    "            st.write(f\"   {col}: {val}\")\n",
    "        \n",
    "        st.write(\"Preprocessing successful!\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        st.error(f\"Preprocessing error: {str(e)}\")\n",
    "        st.exception(e)\n",
    "        return None\n",
    "\n",
    "# NEW: Function to analyze feature impact\n",
    "def analyze_feature_impact(processed_data, pipeline):\n",
    "    \"\"\"Analyze which features are contributing most to the prediction\"\"\"\n",
    "    try:\n",
    "        # Get feature importance if available\n",
    "        if hasattr(pipeline['model'], 'feature_importances_'):\n",
    "            feature_importance = pipeline['model'].feature_importances_\n",
    "            feature_names = pipeline['preprocessing_info']['final_feature_names']\n",
    "            \n",
    "            # Create importance dataframe\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': feature_importance,\n",
    "                'value': processed_data.iloc[0].values\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            st.write(\"**Top 10 Most Important Features:**\")\n",
    "            for _, row in importance_df.head(10).iterrows():\n",
    "                impact = row['importance'] * row['value']\n",
    "                st.write(f\"- {row['feature']}: importance={row['importance']:.3f}, value={row['value']:.3f}, impact={impact:.3f}\")\n",
    "                \n",
    "        elif hasattr(pipeline['model'], 'coef_'):\n",
    "            # For linear models, show coefficients\n",
    "            coefficients = pipeline['model'].coef_[0]\n",
    "            feature_names = pipeline['preprocessing_info']['final_feature_names']\n",
    "            \n",
    "            coef_df = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'coefficient': coefficients,\n",
    "                'value': processed_data.iloc[0].values\n",
    "            })\n",
    "            coef_df['impact'] = coef_df['coefficient'] * coef_df['value']\n",
    "            coef_df = coef_df.sort_values('impact', key=abs, ascending=False)\n",
    "            \n",
    "            st.write(\"**Top 10 Features by Impact (coefficient * value):**\")\n",
    "            for _, row in coef_df.head(10).iterrows():\n",
    "                direction = \"↑\" if row['impact'] > 0 else \"↓\"\n",
    "                st.write(f\"- {row['feature']}: coef={row['coefficient']:.3f}, value={row['value']:.3f}, impact={row['impact']:.3f} {direction}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        st.write(f\"Could not analyze feature impact: {str(e)}\")\n",
    "\n",
    "# NEW: Test different values function\n",
    "def test_value_impact(base_input, pipeline, feature_name, test_values):\n",
    "    \"\"\"Test how changing a specific feature affects prediction\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for test_value in test_values:\n",
    "        # Create test input\n",
    "        test_input = base_input.copy()\n",
    "        test_input[feature_name] = [test_value]\n",
    "        \n",
    "        try:\n",
    "            # Make prediction\n",
    "            result = pipeline['predict_function'](test_input, 'churn_complete_model.joblib')\n",
    "            results.append({\n",
    "                'value': test_value,\n",
    "                'churn_prob': result['churn_probabilities'][0]\n",
    "            })\n",
    "        except Exception as e:\n",
    "            st.write(f\"Error testing {feature_name}={test_value}: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# NEW: Test individual feature impact in isolation\n",
    "def test_monthly_charges_isolation(pipeline):\n",
    "    \"\"\"Test monthly charges impact with all other features fixed\"\"\"\n",
    "    # Create baseline customer\n",
    "    baseline = pd.DataFrame({\n",
    "        'Customer ID': ['TEST'],\n",
    "        'Senior Citizen': ['No'],\n",
    "        'Partner': ['No'], \n",
    "        'Dependents': ['No'],\n",
    "        'Tenure': [12],  # Fixed tenure\n",
    "        'Phone Service': ['Yes'],\n",
    "        'Multiple Lines': ['No'],\n",
    "        'Internet Service': ['DSL'],\n",
    "        'Online Security': ['No'],\n",
    "        'Online Backup': ['No'],\n",
    "        'Device Protection': ['No'],\n",
    "        'Tech Support': ['No'],\n",
    "        'Streaming TV': ['No'],\n",
    "        'Streaming Movies': ['No'],\n",
    "        'Paperless Billing': ['No'],\n",
    "        'Contract': ['Month-to-month'],  # Fixed contract\n",
    "        'Payment Method': ['Electronic check'],\n",
    "        'Monthly Charges': [50.0],  # Will vary this\n",
    "        'Total Charges': [600.0]  # Fixed total\n",
    "    })\n",
    "    \n",
    "    st.write(\"**Monthly Charges Impact Test (all other features fixed):**\")\n",
    "    \n",
    "    # Test different monthly charges\n",
    "    for charges in [20, 40, 60, 80, 100]:\n",
    "        test_data = baseline.copy()\n",
    "        test_data['Monthly Charges'] = [charges]\n",
    "        \n",
    "        try:\n",
    "            result = pipeline['predict_function'](test_data, 'churn_complete_model.joblib')\n",
    "            st.write(f\"Monthly Charges ${charges}: {result['churn_probabilities'][0]:.1%}\")\n",
    "        except Exception as e:\n",
    "            st.write(f\"Error testing Monthly Charges ${charges}: {str(e)}\")\n",
    "\n",
    "# Main app\n",
    "def main():\n",
    "    st.title(\"🔮 Customer Churn Prediction\")\n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    # Load pipeline\n",
    "    pipeline = load_model()\n",
    "    \n",
    "    if pipeline is None:\n",
    "        st.stop()\n",
    "    \n",
    "    # Debug toggle\n",
    "    debug_mode = st.sidebar.checkbox(\"Debug Mode\", value=False)\n",
    "    test_mode = st.sidebar.checkbox(\"Test Feature Impact\", value=False)\n",
    "    \n",
    "    # Create two columns\n",
    "    col1, col2 = st.columns([2, 1])\n",
    "    \n",
    "    with col1:\n",
    "        st.subheader(\"Customer Information\")\n",
    "        \n",
    "        # Create input form\n",
    "        with st.form(\"prediction_form\"):\n",
    "            # Customer demographics\n",
    "            st.write(\"**Demographics**\")\n",
    "            col_demo1, col_demo2 = st.columns(2)\n",
    "            \n",
    "            with col_demo1:\n",
    "                senior_citizen = st.selectbox(\"Senior Citizen\", [\"No\", \"Yes\"])\n",
    "                partner = st.selectbox(\"Partner\", [\"No\", \"Yes\"])\n",
    "                dependents = st.selectbox(\"Dependents\", [\"No\", \"Yes\"])\n",
    "            \n",
    "            with col_demo2:\n",
    "                tenure = st.number_input(\"Tenure (months)\", min_value=0, max_value=100, value=12)\n",
    "                phone_service = st.selectbox(\"Phone Service\", [\"No\", \"Yes\"])\n",
    "                multiple_lines = st.selectbox(\"Multiple Lines\", [\"No\", \"Yes\", \"No phone service\"])\n",
    "            \n",
    "            # Services\n",
    "            st.write(\"**Services**\")\n",
    "            col_serv1, col_serv2 = st.columns(2)\n",
    "            \n",
    "            with col_serv1:\n",
    "                internet_service = st.selectbox(\"Internet Service\", [\"DSL\", \"Fiber optic\", \"No\"])\n",
    "                online_security = st.selectbox(\"Online Security\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                online_backup = st.selectbox(\"Online Backup\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                device_protection = st.selectbox(\"Device Protection\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "            \n",
    "            with col_serv2:\n",
    "                tech_support = st.selectbox(\"Tech Support\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                streaming_tv = st.selectbox(\"Streaming TV\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                streaming_movies = st.selectbox(\"Streaming Movies\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                paperless_billing = st.selectbox(\"Paperless Billing\", [\"No\", \"Yes\"])\n",
    "            \n",
    "            # Contract and payment\n",
    "            st.write(\"**Contract & Payment**\")\n",
    "            col_pay1, col_pay2 = st.columns(2)\n",
    "            \n",
    "            with col_pay1:\n",
    "                contract = st.selectbox(\"Contract\", [\"Month-to-month\", \"One year\", \"Two year\"])\n",
    "                payment_method = st.selectbox(\"Payment Method\", \n",
    "                    [\"Electronic check\", \"Mailed check\", \"Bank transfer (automatic)\", \"Credit card (automatic)\"])\n",
    "            \n",
    "            with col_pay2:\n",
    "                monthly_charges = st.number_input(\"Monthly Charges ($)\", \n",
    "                    min_value=18.0, max_value=120.0, value=50.0, step=0.25)\n",
    "                total_charges = st.number_input(\"Total Charges ($)\", \n",
    "                    min_value=18.0, max_value=9000.0, value=500.0, step=0.1)\n",
    "            \n",
    "            # Submit button\n",
    "            submitted = st.form_submit_button(\"🔍 Predict Churn\", use_container_width=True)\n",
    "            \n",
    "            if submitted:\n",
    "                # Create input dataframe\n",
    "                input_data = pd.DataFrame({\n",
    "                    'Customer ID': ['TEST_001'],  # Dummy ID\n",
    "                    'Senior Citizen': [senior_citizen],\n",
    "                    'Partner': [partner],\n",
    "                    'Dependents': [dependents],\n",
    "                    'Tenure': [tenure],\n",
    "                    'Phone Service': [phone_service],\n",
    "                    'Multiple Lines': [multiple_lines],\n",
    "                    'Internet Service': [internet_service],\n",
    "                    'Online Security': [online_security],\n",
    "                    'Online Backup': [online_backup],\n",
    "                    'Device Protection': [device_protection],\n",
    "                    'Tech Support': [tech_support],\n",
    "                    'Streaming TV': [streaming_tv],\n",
    "                    'Streaming Movies': [streaming_movies],\n",
    "                    'Paperless Billing': [paperless_billing],\n",
    "                    'Contract': [contract],\n",
    "                    'Payment Method': [payment_method],\n",
    "                    'Monthly Charges': [monthly_charges],\n",
    "                    'Total Charges': [total_charges]\n",
    "                })\n",
    "                \n",
    "                # Debug preprocessing if enabled\n",
    "                processed_data = None\n",
    "                if debug_mode:\n",
    "                    processed_data = debug_preprocessing(input_data, pipeline)\n",
    "                \n",
    "                try:\n",
    "                    # Make prediction using your pipeline\n",
    "                    result = pipeline['predict_function'](input_data, 'churn_complete_model.joblib')\n",
    "                    \n",
    "                    # Display results in the second column\n",
    "                    with col2:\n",
    "                        st.subheader(\"Prediction Results\")\n",
    "                        \n",
    "                        churn_prob = result['churn_probabilities'][0]\n",
    "                        prediction = result['predictions'][0]\n",
    "                        \n",
    "                        # Show probability\n",
    "                        st.metric(\n",
    "                            label=\"Churn Probability\",\n",
    "                            value=f\"{churn_prob:.1%}\",\n",
    "                            delta=f\"{'High Risk' if churn_prob > 0.5 else 'Low Risk'}\"\n",
    "                        )\n",
    "                        \n",
    "                        # Show prediction\n",
    "                        if prediction == 1:\n",
    "                            st.error(\"🚨 **LIKELY TO CHURN**\")\n",
    "                            st.write(\"This customer has a high probability of churning.\")\n",
    "                        else:\n",
    "                            st.success(\"✅ **LIKELY TO STAY**\")\n",
    "                            st.write(\"This customer has a low probability of churning.\")\n",
    "                        \n",
    "                        # Progress bar\n",
    "                        st.write(\"**Risk Level:**\")\n",
    "                        st.progress(churn_prob)\n",
    "                        \n",
    "                        # Feature impact analysis\n",
    "                        if debug_mode and processed_data is not None:\n",
    "                            analyze_feature_impact(processed_data, pipeline)\n",
    "                        \n",
    "                        # Test feature impact\n",
    "                        if test_mode:\n",
    "                            st.write(\"**Testing Monthly Charges Impact:**\")\n",
    "                            test_results = test_value_impact(\n",
    "                                input_data, pipeline, 'Monthly Charges', \n",
    "                                [20, 40, 60, 80, 100]\n",
    "                            )\n",
    "                            for result in test_results:\n",
    "                                st.write(f\"Monthly Charges ${result['value']}: {result['churn_prob']:.1%}\")\n",
    "                            \n",
    "                            # Test in isolation\n",
    "                            test_monthly_charges_isolation(pipeline)\n",
    "                        \n",
    "                        # IMPROVED: More accurate insights based on actual model behavior\n",
    "                        st.write(\"**Analysis Notes:**\")\n",
    "                        st.write(\"- If higher charges decrease churn probability, the model may have learned unexpected patterns\")\n",
    "                        st.write(\"- Check if preprocessing is correct (scaling, encoding)\")\n",
    "                        st.write(\"- Verify model training data distribution\")\n",
    "                        st.write(\"- Consider feature interactions in the model\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    st.error(f\"Prediction error: {str(e)}\")\n",
    "                    st.write(\"Please check your model pipeline and input data format.\")\n",
    "                    \n",
    "                    # Show more detailed error info\n",
    "                    if debug_mode:\n",
    "                        st.write(\"**Error details:**\")\n",
    "                        st.exception(e)\n",
    "    \n",
    "    # Footer\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        <div style='text-align: center; color: #666666;'>\n",
    "            <p>Built with Streamlit | Customer Churn Prediction Model</p>\n",
    "        </div>\n",
    "        \"\"\", \n",
    "        unsafe_allow_html=True\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save app.py\n",
    "with open('app.py', 'w') as f:\n",
    "    f.write(debuging_app3)\n",
    "print(\"app.py created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4064b8cb-a694-4bc0-bf2c-012a45dcb45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app.py created\n"
     ]
    }
   ],
   "source": [
    "final_app = '''import streamlit as st\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "\n",
    "# Page config\n",
    "st.set_page_config(\n",
    "    page_title=\"Churn Prediction App\",\n",
    "    page_icon=\"📊\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "def preprocess_new_data(new_data, preprocessing_info):\n",
    "    \"\"\"\n",
    "    Preprocess new data the same way as train data - IMPROVED VERSION\n",
    "    \"\"\"\n",
    "    df = new_data.copy()\n",
    "    \n",
    "    # Remove ID cols\n",
    "    df = df.drop(columns=[col for col in preprocessing_info['id_cols'] if col in df.columns], errors='ignore')\n",
    "    \n",
    "    # CRITICAL FIX: Apply label encoding for binary columns FIRST\n",
    "    for col in preprocessing_info['bin_cols']:\n",
    "        if col in df.columns and col != 'Churn':  # exclude target if present\n",
    "            if col in preprocessing_info['label_encoders']:\n",
    "                # Use the saved encoder from training\n",
    "                try:\n",
    "                    df[col] = preprocessing_info['label_encoders'][col].transform(df[col].astype(str))\n",
    "                except ValueError as e:\n",
    "                    st.warning(f\"Unknown category in {col}: {df[col].unique()}\")\n",
    "                    # Handle unknown categories by mapping to most frequent class\n",
    "                    df[col] = df[col].map({'No': 0, 'Yes': 1}).fillna(0)\n",
    "            else:\n",
    "                # Fallback - create mapping manually\n",
    "                df[col] = df[col].map({'No': 0, 'Yes': 1}).fillna(0)\n",
    "    \n",
    "    # Apply get_dummies for multi-category columns\n",
    "    df = pd.get_dummies(data=df, columns=preprocessing_info['multi_cols'], drop_first=False)\n",
    "    \n",
    "    # CRITICAL FIX: Apply scaling AFTER categorical encoding\n",
    "    if preprocessing_info['num_cols']:\n",
    "        # Apply scaling to ALL numerical columns at once (same as training)\n",
    "        num_cols_present = [col for col in preprocessing_info['num_cols'] if col in df.columns]\n",
    "        if num_cols_present:\n",
    "            # Make sure we scale only the numerical columns that still exist\n",
    "            scaled_values = preprocessing_info['scaler'].transform(df[num_cols_present])\n",
    "            df[num_cols_present] = scaled_values\n",
    "    \n",
    "    # Make sure we have all dummy columns (add missing ones with 0 values)\n",
    "    for col in preprocessing_info['final_feature_names']:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "    \n",
    "    # Ensure correct column order\n",
    "    df = df[preprocessing_info['final_feature_names']]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def predict_churn(input_data, pipeline_path='churn_model_pipeline.joblib'):\n",
    "    \"\"\"\n",
    "    Complete predict function - from raw data to result\n",
    "    \"\"\"\n",
    "    # Load pipeline\n",
    "    pipeline = joblib.load(pipeline_path)\n",
    "    \n",
    "    # Preprocess data\n",
    "    processed_data = preprocess_new_data(input_data, pipeline['preprocessing_info'])\n",
    "    \n",
    "    # Predict\n",
    "    probabilities = pipeline['model'].predict_proba(processed_data)\n",
    "    churn_probability = probabilities[:, 1]  # probability of class 1 (churn)\n",
    "    \n",
    "    # Apply threshold\n",
    "    predictions = (churn_probability >= pipeline['model_info']['threshold']).astype(int)\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'churn_probabilities': churn_probability,\n",
    "        'no_churn_probabilities': probabilities[:, 0]\n",
    "    }\n",
    "\n",
    "# Load model\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    try:\n",
    "        return joblib.load('churn_complete_model.joblib')\n",
    "    except FileNotFoundError:\n",
    "        st.error(\"Model file not found. Please upload churn_complete_model.joblib to your repository.\")\n",
    "        return None\n",
    "\n",
    "# Enhanced debug function with feature importance\n",
    "def debug_preprocessing(input_data, pipeline):\n",
    "    \"\"\"Enhanced debug function to check preprocessing and feature impact\"\"\"\n",
    "    st.write(\"**Debug Information:**\")\n",
    "    \n",
    "    # Show original input\n",
    "    st.write(\"Original input columns:\")\n",
    "    st.write(list(input_data.columns))\n",
    "    \n",
    "    # Show preprocessing info\n",
    "    st.write(\"Expected numerical columns:\")\n",
    "    st.write(pipeline['preprocessing_info']['num_cols'])\n",
    "    \n",
    "    st.write(\"Expected binary columns:\")\n",
    "    st.write(pipeline['preprocessing_info']['bin_cols'])\n",
    "    \n",
    "    st.write(\"Expected multi-category columns:\")\n",
    "    st.write(pipeline['preprocessing_info']['multi_cols'])\n",
    "    \n",
    "    # Show preprocessing steps\n",
    "    try:\n",
    "        # Step by step preprocessing\n",
    "        df = input_data.copy()\n",
    "        st.write(f\"1. After copy: {df.shape}\")\n",
    "        \n",
    "        # Remove ID cols\n",
    "        df = df.drop(columns=[col for col in pipeline['preprocessing_info']['id_cols'] if col in df.columns], errors='ignore')\n",
    "        st.write(f\"2. After removing ID cols: {df.shape}\")\n",
    "        \n",
    "        # Binary encoding FIRST\n",
    "        for col in pipeline['preprocessing_info']['bin_cols']:\n",
    "            if col in df.columns and col != 'Churn':\n",
    "                if col in pipeline['preprocessing_info']['label_encoders']:\n",
    "                    df[col] = pipeline['preprocessing_info']['label_encoders'][col].transform(df[col].astype(str))\n",
    "                else:\n",
    "                    df[col] = df[col].map({'No': 0, 'Yes': 1}).fillna(0)\n",
    "        st.write(f\"3. After binary encoding: {df.shape}\")\n",
    "        \n",
    "        # Get dummies\n",
    "        df = pd.get_dummies(data=df, columns=pipeline['preprocessing_info']['multi_cols'], drop_first=False)\n",
    "        st.write(f\"4. After get_dummies: {df.shape}\")\n",
    "        st.write(f\"   Columns after get_dummies: {list(df.columns)}\")\n",
    "        \n",
    "        # Scale numerical columns AFTER categorical\n",
    "        if pipeline['preprocessing_info']['num_cols']:\n",
    "            num_cols_present = [col for col in pipeline['preprocessing_info']['num_cols'] if col in df.columns]\n",
    "            st.write(f\"5. Numerical columns present: {num_cols_present}\")\n",
    "            if num_cols_present:\n",
    "                st.write(f\"   Values before scaling: {df[num_cols_present].iloc[0].to_dict()}\")\n",
    "                scaled_values = pipeline['preprocessing_info']['scaler'].transform(df[num_cols_present])\n",
    "                df[num_cols_present] = scaled_values\n",
    "                st.write(f\"   Values after scaling: {df[num_cols_present].iloc[0].to_dict()}\")\n",
    "                st.write(f\"6. After scaling: {df.shape}\")\n",
    "        \n",
    "        # Add missing columns\n",
    "        missing_cols = []\n",
    "        for col in pipeline['preprocessing_info']['final_feature_names']:\n",
    "            if col not in df.columns:\n",
    "                df[col] = 0\n",
    "                missing_cols.append(col)\n",
    "        \n",
    "        if missing_cols:\n",
    "            st.write(f\"7. Added missing columns: {len(missing_cols)}\")\n",
    "            st.write(f\"   Missing columns: {missing_cols[:10]}...\")  # Show first 10\n",
    "        \n",
    "        # Final ordering\n",
    "        df = df[pipeline['preprocessing_info']['final_feature_names']]\n",
    "        st.write(f\"8. Final shape: {df.shape}\")\n",
    "        \n",
    "        # Show final processed values\n",
    "        st.write(\"**Final processed values (first 10 features):**\")\n",
    "        final_values = df.iloc[0].to_dict()\n",
    "        for i, (col, val) in enumerate(list(final_values.items())[:10]):\n",
    "            st.write(f\"   {col}: {val}\")\n",
    "        \n",
    "        st.write(\"Preprocessing successful!\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        st.error(f\"Preprocessing error: {str(e)}\")\n",
    "        st.exception(e)\n",
    "        return None\n",
    "\n",
    "# Function to analyze feature impact\n",
    "def analyze_feature_impact(processed_data, pipeline):\n",
    "    \"\"\"Analyze which features are contributing most to the prediction\"\"\"\n",
    "    try:\n",
    "        # Get feature importance if available\n",
    "        if hasattr(pipeline['model'], 'feature_importances_'):\n",
    "            feature_importance = pipeline['model'].feature_importances_\n",
    "            feature_names = pipeline['preprocessing_info']['final_feature_names']\n",
    "            \n",
    "            # Create importance dataframe\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': feature_importance,\n",
    "                'value': processed_data.iloc[0].values\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            st.write(\"**Top 10 Most Important Features:**\")\n",
    "            for _, row in importance_df.head(10).iterrows():\n",
    "                impact = row['importance'] * row['value']\n",
    "                st.write(f\"- {row['feature']}: importance={row['importance']:.3f}, value={row['value']:.3f}, impact={impact:.3f}\")\n",
    "                \n",
    "        elif hasattr(pipeline['model'], 'coef_'):\n",
    "            # For linear models, show coefficients\n",
    "            coefficients = pipeline['model'].coef_[0]\n",
    "            feature_names = pipeline['preprocessing_info']['final_feature_names']\n",
    "            \n",
    "            coef_df = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'coefficient': coefficients,\n",
    "                'value': processed_data.iloc[0].values\n",
    "            })\n",
    "            coef_df['impact'] = coef_df['coefficient'] * coef_df['value']\n",
    "            coef_df = coef_df.sort_values('impact', key=abs, ascending=False)\n",
    "            \n",
    "            st.write(\"**Top 10 Features by Impact (coefficient * value):**\")\n",
    "            for _, row in coef_df.head(10).iterrows():\n",
    "                direction = \"↑\" if row['impact'] > 0 else \"↓\"\n",
    "                st.write(f\"- {row['feature']}: coef={row['coefficient']:.3f}, value={row['value']:.3f}, impact={row['impact']:.3f} {direction}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        st.write(f\"Could not analyze feature impact: {str(e)}\")\n",
    "\n",
    "# NEW: Test different values function\n",
    "def test_value_impact(base_input, pipeline, feature_name, test_values):\n",
    "    \"\"\"Test how changing a specific feature affects prediction\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for test_value in test_values:\n",
    "        # Create test input\n",
    "        test_input = base_input.copy()\n",
    "        test_input[feature_name] = [test_value]\n",
    "        \n",
    "        try:\n",
    "            # Make prediction\n",
    "            result = pipeline['predict_function'](test_input, 'churn_complete_model.joblib')\n",
    "            results.append({\n",
    "                'value': test_value,\n",
    "                'churn_prob': result['churn_probabilities'][0]\n",
    "            })\n",
    "        except Exception as e:\n",
    "            st.write(f\"Error testing {feature_name}={test_value}: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test individual feature impact in isolation\n",
    "def test_monthly_charges_isolation(pipeline):\n",
    "    \"\"\"Test monthly charges impact with all other features fixed\"\"\"\n",
    "    # Create baseline customer\n",
    "    baseline = pd.DataFrame({\n",
    "        'Customer ID': ['TEST'],\n",
    "        'Senior Citizen': ['No'],\n",
    "        'Partner': ['No'], \n",
    "        'Dependents': ['No'],\n",
    "        'Tenure': [12],  # Fixed tenure\n",
    "        'Phone Service': ['Yes'],\n",
    "        'Multiple Lines': ['No'],\n",
    "        'Internet Service': ['DSL'],\n",
    "        'Online Security': ['No'],\n",
    "        'Online Backup': ['No'],\n",
    "        'Device Protection': ['No'],\n",
    "        'Tech Support': ['No'],\n",
    "        'Streaming TV': ['No'],\n",
    "        'Streaming Movies': ['No'],\n",
    "        'Paperless Billing': ['No'],\n",
    "        'Contract': ['Month-to-month'],  # Fixed contract\n",
    "        'Payment Method': ['Electronic check'],\n",
    "        'Monthly Charges': [50.0],  # Will vary this\n",
    "        'Total Charges': [600.0]  # Fixed total\n",
    "    })\n",
    "    \n",
    "    st.write(\"**Monthly Charges Impact Test (all other features fixed):**\")\n",
    "    \n",
    "    # Test different monthly charges\n",
    "    for charges in [20, 40, 60, 80, 100]:\n",
    "        test_data = baseline.copy()\n",
    "        test_data['Monthly Charges'] = [charges]\n",
    "        \n",
    "        try:\n",
    "            result = pipeline['predict_function'](test_data, 'churn_complete_model.joblib')\n",
    "            st.write(f\"Monthly Charges ${charges}: {result['churn_probabilities'][0]:.1%}\")\n",
    "        except Exception as e:\n",
    "            st.write(f\"Error testing Monthly Charges ${charges}: {str(e)}\")\n",
    "\n",
    "# Main app\n",
    "def main():\n",
    "    st.title(\"🔮 Customer Churn Prediction\")\n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    # Load pipeline\n",
    "    pipeline = load_model()\n",
    "    \n",
    "    if pipeline is None:\n",
    "        st.stop()\n",
    "    \n",
    "    # Debug toggle\n",
    "    debug_mode = st.sidebar.checkbox(\"Debug Mode\", value=False)\n",
    "    test_mode = st.sidebar.checkbox(\"Test Feature Impact\", value=False)\n",
    "    \n",
    "    # Create two columns\n",
    "    col1, col2 = st.columns([2, 1])\n",
    "    \n",
    "    with col1:\n",
    "        st.subheader(\"Customer Information\")\n",
    "        \n",
    "        # Create input form\n",
    "        with st.form(\"prediction_form\"):\n",
    "            # Customer demographics\n",
    "            st.write(\"**Demographics**\")\n",
    "            col_demo1, col_demo2 = st.columns(2)\n",
    "            \n",
    "            with col_demo1:\n",
    "                senior_citizen = st.selectbox(\"Senior Citizen\", [\"No\", \"Yes\"])\n",
    "                partner = st.selectbox(\"Partner\", [\"No\", \"Yes\"])\n",
    "                dependents = st.selectbox(\"Dependents\", [\"No\", \"Yes\"])\n",
    "            \n",
    "            with col_demo2:\n",
    "                tenure = st.number_input(\"Tenure (months)\", min_value=0, max_value=100, value=12)\n",
    "                phone_service = st.selectbox(\"Phone Service\", [\"No\", \"Yes\"])\n",
    "                multiple_lines = st.selectbox(\"Multiple Lines\", [\"No\", \"Yes\", \"No phone service\"])\n",
    "            \n",
    "            # Services\n",
    "            st.write(\"**Services**\")\n",
    "            col_serv1, col_serv2 = st.columns(2)\n",
    "            \n",
    "            with col_serv1:\n",
    "                internet_service = st.selectbox(\"Internet Service\", [\"DSL\", \"Fiber optic\", \"No\"])\n",
    "                online_security = st.selectbox(\"Online Security\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                online_backup = st.selectbox(\"Online Backup\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                device_protection = st.selectbox(\"Device Protection\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "            \n",
    "            with col_serv2:\n",
    "                tech_support = st.selectbox(\"Tech Support\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                streaming_tv = st.selectbox(\"Streaming TV\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                streaming_movies = st.selectbox(\"Streaming Movies\", [\"No\", \"Yes\", \"No internet service\"])\n",
    "                paperless_billing = st.selectbox(\"Paperless Billing\", [\"No\", \"Yes\"])\n",
    "            \n",
    "            # Contract and payment\n",
    "            st.write(\"**Contract & Payment**\")\n",
    "            col_pay1, col_pay2 = st.columns(2)\n",
    "            \n",
    "            with col_pay1:\n",
    "                contract = st.selectbox(\"Contract\", [\"Month-to-month\", \"One year\", \"Two year\"])\n",
    "                payment_method = st.selectbox(\"Payment Method\", \n",
    "                    [\"Electronic check\", \"Mailed check\", \"Bank transfer (automatic)\", \"Credit card (automatic)\"])\n",
    "            \n",
    "            with col_pay2:\n",
    "                monthly_charges = st.number_input(\"Monthly Charges ($)\", \n",
    "                    min_value=18.0, max_value=120.0, value=50.0, step=0.25)\n",
    "                total_charges = st.number_input(\"Total Charges ($)\", \n",
    "                    min_value=18.0, max_value=9000.0, value=500.0, step=0.1)\n",
    "            \n",
    "            # Submit button\n",
    "            submitted = st.form_submit_button(\"🔍 Predict Churn\", use_container_width=True)\n",
    "            \n",
    "            if submitted:\n",
    "                # Create input dataframe\n",
    "                input_data = pd.DataFrame({\n",
    "                    'Customer ID': ['TEST_001'],  # Dummy ID\n",
    "                    'Senior Citizen': [senior_citizen],\n",
    "                    'Partner': [partner],\n",
    "                    'Dependents': [dependents],\n",
    "                    'Tenure': [tenure],\n",
    "                    'Phone Service': [phone_service],\n",
    "                    'Multiple Lines': [multiple_lines],\n",
    "                    'Internet Service': [internet_service],\n",
    "                    'Online Security': [online_security],\n",
    "                    'Online Backup': [online_backup],\n",
    "                    'Device Protection': [device_protection],\n",
    "                    'Tech Support': [tech_support],\n",
    "                    'Streaming TV': [streaming_tv],\n",
    "                    'Streaming Movies': [streaming_movies],\n",
    "                    'Paperless Billing': [paperless_billing],\n",
    "                    'Contract': [contract],\n",
    "                    'Payment Method': [payment_method],\n",
    "                    'Monthly Charges': [monthly_charges],\n",
    "                    'Total Charges': [total_charges]\n",
    "                })\n",
    "                \n",
    "                # Debug preprocessing if enabled\n",
    "                processed_data = None\n",
    "                if debug_mode:\n",
    "                    processed_data = debug_preprocessing(input_data, pipeline)\n",
    "                \n",
    "                try:\n",
    "                    # Make prediction using your pipeline\n",
    "                    result = pipeline['predict_function'](input_data, 'churn_complete_model.joblib')\n",
    "                    \n",
    "                    # Display results in the second column\n",
    "                    with col2:\n",
    "                        st.subheader(\"Prediction Results\")\n",
    "                        \n",
    "                        churn_prob = result['churn_probabilities'][0]\n",
    "                        prediction = result['predictions'][0]\n",
    "                        \n",
    "                        # Show probability\n",
    "                        st.metric(\n",
    "                            label=\"Churn Probability\",\n",
    "                            value=f\"{churn_prob:.1%}\",\n",
    "                            delta=f\"{'High Risk' if churn_prob > 0.5 else 'Low Risk'}\"\n",
    "                        )\n",
    "                        \n",
    "                        # Show prediction\n",
    "                        if prediction == 1:\n",
    "                            st.error(\"🚨 **LIKELY TO CHURN**\")\n",
    "                            st.write(\"This customer has a high probability of churning.\")\n",
    "                        else:\n",
    "                            st.success(\"✅ **LIKELY TO STAY**\")\n",
    "                            st.write(\"This customer has a low probability of churning.\")\n",
    "                        \n",
    "                        # Progress bar\n",
    "                        st.write(\"**Risk Level:**\")\n",
    "                        st.progress(churn_prob)\n",
    "                        \n",
    "                        # Feature impact analysis\n",
    "                        if debug_mode and processed_data is not None:\n",
    "                            analyze_feature_impact(processed_data, pipeline)\n",
    "                        \n",
    "                        # Test feature impact\n",
    "                        if test_mode:\n",
    "                            st.write(\"**Testing Monthly Charges Impact:**\")\n",
    "                            test_results = test_value_impact(\n",
    "                                input_data, pipeline, 'Monthly Charges', \n",
    "                                [20, 40, 60, 80, 100]\n",
    "                            )\n",
    "                            for result in test_results:\n",
    "                                st.write(f\"Monthly Charges ${result['value']}: {result['churn_prob']:.1%}\")\n",
    "                            \n",
    "                            # Test in isolation\n",
    "                            test_monthly_charges_isolation(pipeline)\n",
    "                        \n",
    "                \n",
    "                except Exception as e:\n",
    "                    st.error(f\"Prediction error: {str(e)}\")\n",
    "                    st.write(\"Please check your model pipeline and input data format.\")\n",
    "                    \n",
    "                    # Show more detailed error info\n",
    "                    if debug_mode:\n",
    "                        st.write(\"**Error details:**\")\n",
    "                        st.exception(e)\n",
    "    \n",
    "    # Footer\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        <div style='text-align: center; color: #666666;'>\n",
    "            <p>Built with Streamlit | Customer Churn Prediction Model</p>\n",
    "        </div>\n",
    "        \"\"\", \n",
    "        unsafe_allow_html=True\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save app.py\n",
    "with open('app.py', 'w') as f:\n",
    "    f.write(final_app)\n",
    "print(\"app.py created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bef424a2-bee7-4927-86c1-1a629ed5fbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "churndf = pd.read_csv('CustomerChurn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6aa7a72-a152-4697-914a-2964da81a7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7032.000000\n",
       "mean     2283.300441\n",
       "std      2266.771362\n",
       "min        18.800000\n",
       "25%       401.450000\n",
       "50%      1397.475000\n",
       "75%      3794.737500\n",
       "max      8684.800000\n",
       "Name: Total Charges, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churndf['Total Charges'] = churndf['Total Charges'].replace(\" \",np.nan)\n",
    "churndf = churndf.dropna(subset=['Total Charges'])\n",
    "churndf['Total Charges'].astype(float).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c07e1114-552d-4494-9560-b90ad0ee64ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LoyaltyID', 'Customer ID', 'Senior Citizen', 'Partner', 'Dependents',\n",
       "       'Tenure', 'Phone Service', 'Multiple Lines', 'Internet Service',\n",
       "       'Online Security', 'Online Backup', 'Device Protection', 'Tech Support',\n",
       "       'Streaming TV', 'Streaming Movies', 'Contract', 'Paperless Billing',\n",
       "       'Payment Method', 'Monthly Charges', 'Total Charges', 'Churn'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churndf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29f5a2bf-51dd-4c2a-94ff-d66b5056467c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      29.85\n",
       "1     1889.5\n",
       "2     108.15\n",
       "3    1840.75\n",
       "4     151.65\n",
       "Name: Total Charges, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churndf['Total Charges'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acaad8a3-a19d-435b-aa1f-9881aa820046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7032.000000\n",
       "mean       64.798208\n",
       "std        30.085974\n",
       "min        18.250000\n",
       "25%        35.587500\n",
       "50%        70.350000\n",
       "75%        89.862500\n",
       "max       118.750000\n",
       "Name: Monthly Charges, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churndf['Monthly Charges'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e999f7b1-c85e-4cac-a260-d2597fb23257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
